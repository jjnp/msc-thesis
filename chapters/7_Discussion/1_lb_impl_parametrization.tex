\section{Load Balancer Implementation and Parametrization}
% 2-3 pages
\subsection{Choosing the right implementation}
Our evaluations show that careful consideration is necesssary when deciding on a load balancer implementation.
Our results show that some implementations can exhibit idiosyncratic behaviour that might be undesirable for the FaaS system in general.

Our implementation considerations were focused in large parts on how the weighted round robin component of our approach is implemented.
A key result is that the implementation strongly affects how quickly the load balancer is able to discover enough nodes to make an informed load balancing decision and thus converge onto a somewhat stable response time.
From the results we also learn that a deterministic weighted round robin implementation results in faster discovery of the available nodes, as can also be seen in figure \ref{fig:lb_imp_upstream_coverage}.
The potential difficulty of load balancers discovering nodes in an acceptable time frame in system with very high numbers of nodes or load balancers also supports our decision to already initialize load balancers with weights of neighboring instances to reduce the time until performance stabilizes.

We can also observe that certain implementations may exhibit behaviour that is specific to heterogeneous edge computing environments.
An example of such behaviour is the oscillating performance of the \textit{Adapted Classic} load balancing implementation that can be seen in figure \ref{fig:lb_imp_rt_convergence}.
In this implementation upstreams are always worked through iteratively with faster upstreams being chosen first, before slower upstreams are steadily interleaved in order of their respective weight.
As a result the performance varies with time going through periods where performance is extremely  good, and ones where performance is comparatively poor.
When taking the mean response times over a longer period of time though this implementation performs no worse than the \textit{smooth} implementation we chose for our approach.

While intuitively this behaviour might seem undesirable, it also shows potential for more complex application scenarios.
A load balancer could, for example, try to sample as many upstreams as possible, but only send requests to a certain percentile of the best performing ones.
This approach is similar to the implementation proposed by Cicconetti, Conti, and Passarella \cite{cicconettiDecentralizedFrameworkServerless2020}, where they propose to only include upstreams whose response time is at most double the minimum response time achieved.
Sampling in their approach is handled through a generally fixed rate, but with the addition of exponential back off times for upstreams that continuously fail to make the cut.

In terms of additional features, the stratification of upstreams according to their performance would, for example, allow client-specific \gls{qos} policies to be enforced.
This could be relevant for scenarios where some requests are urgent, while others are not, or where devices have to compete for and bid on their \gls{qos}.

Our results here show that there isn't necessarily a singular, optimal implementation, but that depending on goals and requirements of the system an implementation should be chosen.
It also shows, once again, that even system components as basic as weighted round robin implementations may behave differently in a heterogeneous environment such as edge computing, and that careful evaluations are necessary when reusing components developed for the cloud at the edge.

\subsection{Choices in parametrization}
Parametrization is another area where our results indicate developers can make a choice.
As figures \ref{fig:lb_hyper_scaling_perfspread} and \ref{fig:lb_hyper_weightrange_scaling} show, the choice of parameters for how and when response times are mapped onto weights has a significant impact on response times.
The results show that for any given scenario, there can be a whole set of different configurations that would results in the same performance.
We can, however, also see that parameter choices are not universally wise.
What results in good performance under one set of circumstances can result in poor performance under another.
Figure \ref{fig:lb_hyper_reset_q95} shows this well.
While the configuration with weight updates every 150-200 seconds, and a weight range of 20-30 performs well in the 95th percentile in the range of up to 250 \gls{rps}, once \gls{rps} go beyond 500, it performs much worse.

Because \gls{rps} changes are to be expected in a serverless edge computing system, these results indicate that load balancers potentially have to dynamically adapt their parametrization to keep performance as high as possible.
Our results also show that the higher the \gls{rps} a load balancer experiences, the smaller the set of configurations with near optimal performance becomes.
This is particularly relevant to scenarios with high load and strict \gls{sla} policies, since those circumstances would be most affected by suddenly worsened performance due to high \gls{rps} numbers.
While our experimentation in this area is not extensive enough to arrive at definitive conclusions, it suggests that scenarios in which \gls{rps} varies highly requires very careful investigation of the impact of parametrization.


% large effect of parametrization fig 6.5
% interestingly optimal parametrization not static
% depending on scenario performance will vary -> rps is one such variable! likely to change
% again potential for QoS things
% the higher the rps the fewer good configurations there are -> the stakes are higher