\section{Motivation}
% 1.5 pages
Serverless Computing is a computing paradigm which alleviates application developers from deployment considerations to an unprecedented level. % cite general serverless
In traditional, and even in modern containerized computing environments, developers are required to explicitly specify how their application should be deployed \cite{shiPromiseEdgeComputing2016}, thus requiring a great deal of effort and high level of competency. %todo cite this
With the introduction of edge computing, the adaptation of serverless platforms was proposed as a means to abstract the additional complexity arising from edge computing, allowing one to make use of the advantages and new possibilities of edge computing more readily \cite{nasticServerlessRealTimeData2017}\cite{gliksonDevicelessEdgeComputing2017}.
Edge Computing does, however, have its own range of specific challenges that need to be overcome to make use of it effectively, such as easy programmability by software developers\cite{shiPromiseEdgeComputing2016} and service management across its heterogeneous environment\cite{shiEdgeComputingVisionChallenges2016}.

Serverless platforms typically utilize existing technologies to provide their functionality. Most of all they rely on containerization techniques, and as a result, the techniques used to manage large-scale container deployments.
For this reason, popular serverless platforms such as kubeless\cite{kubeless} and OpenFaaS\cite{openfaas} are built on top of Kubernetes, the de-facto standard container orchestration platform.
Neither these platforms nor the underlying container orchestration technology, are by default suited for the unique conditions posed by edge computing, in particular, increased heterogeneity in both network structure and compute capability.
OpenFaaS has been adapted as a serverless system by Rausch et al. with a focus on optimized container scheduling \cite{skippy}.
While function placement has been studied more extensively, efficient processing of network bound workloads, which are workloads where network transfers make up the dominant part of the overall response time, remains a challenge. For serverless frameworks like OpenFaaS, one of the likely causes is the centralized architecture of it and the underlying container orchestration framework, as outlined by Rausch et al.\cite{skippy}.
This centralization leads to the system routing requests in a highly inefficient manner, thus likely causing the relatively poor response times of network bound workloads. A potential solution outlined by Rausch et al. is to distribute the ingress points throughout the cluster, including to the edge\cite{skippy}.
The ingress points first accept the request and forward it to a running instance of the function, thus acting as a load balancer for incoming requests.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{graphics/graphs/initial_global_trt.png}
    \caption{Kernel Density Estimate of an experiment run. Result shows \gls{trt} of different load balancer implementations in a globally distributed scenario.}
    \label{fig:initial_global_trt}
\end{figure}

Before developing an approach to the range of engineering challenges such a solution requires we decided to perform a preliminary analysis to determine whether distributed load balancers for the serverless system are a viable solution at all, and if so by how much the performance of network bound workloads can be improved.
To this end we used Faas-Sim\cite{faas-sim-github},
a serverless edge computing simulator built to emulate the core concepts of OpenFaaS and Kubernetes. Building on the work done by Philipp Raith \cite{philipp-da},
we extended the simulator with a more realistic and adaptable method for load balancing, allowing different load balancing techniques to be employed, and taking into account function placement and network conditions when simulating requests.%
This preliminary analysis shows an improvement in mean response time of between 81.3\% and 606.9\%, depending on the scenario. Figure \ref{fig:initial_global_trt}
shows the fitted probability density functions of the overall response time in a globally distributed scenario. While we can see in Figure \ref{fig:initial_global_trt} that the usage of a more sophisticated load balancing scheme than round robin does improve the performance, as long as the load balancers are centralized the improvement is not particularly significant.
Similarly we can see that the distribution of load balancers across the entire network yields performance benefits.
Most importantly, however, Figure \ref{fig:initial_global_trt} shows that when distributed placement and sophisticated load balancing decisions are used together, the performance improvements are much bigger than either of the two measures can provide on its own.
We also observe that aside from the mean response time being improved, the variance is also reduced, leading to more stable and predictable processing times.
A more detailed account of these first experiments can be found in the evaluation chapter of this thesis.

From the combined results of the initial evaluation we learn that:
\begin{enumerate}
    \item distribution of load balancers alone is insufficient to improve performance,
    \item more sophisticated load balancing methods are required to make use of network proximity, and
    \item sophisticated load balancing methods can, in addition to improving network induced delay, reduce function execution time.
    \item The location of load balancers has a significant impact on performance, making the effective placement of load balancers important
\end{enumerate}
Based on these results we believe that it is worthwhile to explore this area further, and in more detail than the preliminary evaluation could.

