
@misc{openfaas-gateway,
  title = {{{OpenFaaS Architecture Documentation}} - {{Gateway}}},
  author = {OpenFaaS Authors},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://docs.openfaas.com/architecture/gateway/}}
}



@misc{containerd,
  title = {Containerd},
  author = {{containerd Authors}},
  abstract = {An industry-standard container runtime with an emphasis on simplicity, robustness, and portability},
  langid = {american},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://containerd.io/}}
}



@misc{kubernetes,
  title = {Kubernetes},
  author = {The Kubernetes Authors and The Linux Foundation},
  journal = {Kubernetes},
  abstract = {Production-Grade Container Orchestration},
  langid = {english},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://kubernetes.io/}}
}



@misc{openwhisk,
  title = {Apache {{OpenWhisk}}},
  author = {The Apache Software Foundation},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://openwhisk.apache.org/}}
}



@misc{kubeless,
  title = {Kubeless},
  author = {Kubeless},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://kubeless.io/}}
}



@misc{openfaas,
  title = {{{OpenFaaS}}},
  author = {OpenFaaS Authors},
  abstract = {Serverless Functions Made Simple with Kubernetes.},
  langid = {english},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://www.openfaas.com/}}
}

@misc{azure-functions,
  title = {Microsoft {{Azure Functions}} - {{Serverless Compute}}},
  author = {Microsoft},
  abstract = {Create event-driven, scalable serverless apps in .NET, Node.js, Python, Java, or PowerShell. Build and debug locally\textemdash deploy and operate at scale in the cloud.},
  langid = {english},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://azure.microsoft.com/en-us/services/functions/}}
}



@misc{aws-lambda,
  title = {{{AWS Lambda}} \textendash{} {{Serverless Compute}}},
  author = {Amazon Web Services Inc.},
  abstract = {AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.},
  langid = {american},
  note = {Accessed 2021-09-30},
  howpublished = {\url{https://aws.amazon.com/lambda/}}
}


@misc{nginx,
  title = {{{NGINX}}},
  author = {F5 Networks Inc.},
  abstract = {NGINX accelerates content and application delivery, improves security, facilitates availability and scalability for the busiest web sites on the Internet},
  langid = {american},
  note = {Accessed 2021-10-05},
  howpublished = {\url{https://www.nginx.com/}}
}

@misc{nginx-wrr,
  title = {{{NGINX}} - Ngx\_http\_upstream\_round\_robin.c},
  author = {Sysoev, Igor},
  year = {2021},
  month = oct,
  abstract = {An official read-only mirror of http://hg.nginx.org/nginx/ which is updated hourly. Pull requests on GitHub cannot be accepted and will be automatically closed. The proper way to submit changes to nginx is via the nginx development mailing list, see http://nginx.org/en/docs/contributing\_changes.html},
  collaborator = {{Nginx Inc.}},
  note = {Accessed 2021-10-05},
  howpublished = {\url{https://github.com/nginx/nginx/blob/e56ba23158b8466d108fd4d571bd7d9a88f2a473/src/http/ngx_http_upstream_round_robin.c}}
}


@article{osmotic-middleware-rausch,
  title = {Osmotic {{Message}}-{{Oriented Middleware}} for the {{Internet}} of {{Things}}},
  author = {Rausch, Thomas and Dustdar, Schahram and Ranjan, Rajiv},
  year = {2018},
  month = mar,
  journal = {IEEE Cloud Computing},
  volume = {5},
  number = {2},
  pages = {17--25},
  issn = {2325-6095},
  doi = {10.1109/MCC.2018.022171663},
  langid = {english}
}


@inproceedings{rausch-ether,
  title = {Synthesizing {{Plausible Infrastructure Configurations}} for {{Evaluating Edge Computing Systems}}},
  booktitle = {3rd \{\vphantom\}{{USENIX}}\vphantom\{\} {{Workshop}} on {{Hot Topics}} in {{Edge Computing}} ({{HotEdge}} 20)},
  author = {Rausch, Thomas and Lachner, Clemens and Frangoudis, Pantelis A. and Raith, Philipp and Dustdar, Schahram},
  year = {2020},
  langid = {english},
  note = {https://www.usenix.org/conference/hotedge20/presentation/rausch}
}


@misc{faas-sim-github,
  title = {Faas-Sim: A Trace-Driven {{Function}}-as-a-{{Service}} Simulator},
  shorttitle = {Faas-Sim},
  author = {{Thomas Rausch} and {Philipp Raith}},
  year = {2021},
  month = nov,
  abstract = {A framework for trace-driven simulation of serverless Function-as-a-Service platforms},
  copyright = {MIT},
  keywords = {edge-computing,python,serverless-computing,simpy,simulation},
  note = {Accessed 2021-11-11},
  howpublished = {\url{https://github.com/edgerun/faas-sim}}
}



@article{skippy,
  title = {Optimized Container Scheduling for Data-Intensive Serverless Edge Computing},
  author = {Rausch, Thomas and Rashed, Alexander and Dustdar, Schahram},
  year = {2021},
  month = jan,
  journal = {Future Generation Computer Systems},
  volume = {114},
  pages = {259--271},
  issn = {0167739X},
  doi = {10.1016/j.future.2020.07.017},
  abstract = {Operating data-intensive applications on edge systems is challenging, due to the extreme workload and device heterogeneity, as well as the geographic dispersion of compute and storage infrastructure. Serverless computing has emerged as a compelling model to manage the complexity of such systems, by decoupling the underlying infrastructure and scaling mechanisms from applications. Although serverless platforms have reached a high level of maturity, we have found several limiting factors that inhibit their use in an edge setting. This paper presents a container scheduling system that enables such platforms to make efficient use of edge infrastructures. Our scheduler makes heuristic trade-offs between data and computation movement, and considers workload-specific compute requirements such as GPU acceleration. Furthermore, we present a method to automatically fine-tune the weights of scheduling constraints to optimize high-level operational objectives such as minimizing task execution time, uplink usage, or cloud execution cost. We implement a prototype that targets the container orchestration system Kubernetes, and deploy it on an edge testbed we have built. We evaluate our system with trace-driven simulations in different infrastructure scenarios, using traces generated from running representative workloads on our testbed. Our results show that (a) our scheduler significantly improves the quality of task placement compared to the state-of-the-art scheduler of Kubernetes, and (b) our method for fine-tuning scheduling parameters helps significantly in meeting operational goals. \textcopyright{} 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
  langid = {english}
}

%todo change this s.t. it isn't a PHD thesis, since it simply isn't
@phdthesis{philipp-da,
  title = {Container {{Scheduling}} on {{Heterogeneous Clusters}} Using {{Machine Learning}}-Based {{Workload Characterization}}},
  author = {Raith, Philipp Alexander and Rausch, Thomas and {Dustdar, Schahram}},
  year = {2021},
  month = feb,
  address = {{Vienna}},
  langid = {english},
  school = {Vienna University of Technology}
}


@phdthesis{thomas-thesis,
  title = {{A Distributed Compute Fabric for Edge Intelligence}},
  author = {Rausch, Thomas and {Dustdar, Schahram}},
  year = {2021},
  month = may,
  langid = {german},
  school = {Vienna University of Technology}
}


@article{shiEdgeComputingVisionChallenges2016,
  title = {Edge {{Computing}}: Vision and {{Challenges}}},
  shorttitle = {Edge {{Computing}}},
  author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
  year = {2016},
  month = oct,
  journal = {IEEE Internet of Things Journal},
  volume = {3},
  number = {5},
  pages = {637--646},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2016.2579198},
  abstract = {The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
  langid = {english}
}


@misc{telemd-github,
  title = {Telemd},
  author = {{Edgerun Authors}},
  year = {2021},
  month = sep,
  abstract = {Daemon for fine-grained system runtime data monitoring},
  keywords = {go,metrics,monitoring},
  note = {Accessed 2021-11-11},
  howpublished = {\url{https://github.com/edgerun/telemd}}
}


@misc{galileo-github,
  title = {Galileo: A Framework for Distributed Load Testing Experiments},
  shorttitle = {Galileo},
  author = {{Rausch, Thomas} and {Raith, Philipp} and {Palecek, Jacob}},
  year = {2021},
  month = sep,
  abstract = {A framework for distributed load testing experiments},
  keywords = {distributed-load-testing,evaluation-framework,load-testing,python},
  note = {Accessed 2021-11-11},
  howpublished = {\url{https://github.com/edgerun/galileo}}
}


@inproceedings{operating-energy-aware-galileo,
  title = {A System for Operating Energy-Aware Cloudlets: Demo},
  shorttitle = {A System for Operating Energy-Aware Cloudlets},
  booktitle = {Proceedings of the 4th {{ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  author = {Rausch, Thomas and Raith, Philipp and Pillai, Padmanabhan and Dustdar, Schahram},
  year = {2019},
  month = nov,
  pages = {307--309},
  publisher = {{ACM}},
  address = {{Arlington Virginia}},
  doi = {10.1145/3318216.3363325},
  abstract = {We present an end-to-end system for operating energy-aware cloudlets with a low-footprint cluster manager and an adaptive client-side load balancing approach. Our system is designed for small-scale high-density compute clusters that host stateless services and have stringent energy resource constraints. It features cluster and service management, runtime monitoring, adaptive load balancing and cluster reconfiguration policies. Furthermore, we present an experimentation and analytics system that allows coordinated execution of complex workload experiments to evaluate different operational strategies.},
  isbn = {978-1-4503-6733-2},
  langid = {english}
}



@misc{traefik,
  title = {Traefik, {{The Cloud Native Application Proxy}}},
  author = {{Traefik Labs}},
  journal = {Traefik Labs: Makes Networking Boring},
  abstract = {Traefik is the leading open-source reverse proxy and load balancer for HTTP and TCP-based applications that is easy, dynamic and full-featured.},
  langid = {english},
  note = {Accessed 2021-11-11},
  howpublished = {\url{https://traefik.io/traefik/}}
}


@misc{traefik-jjnp,
  title = {Jjnp/Traefik},
  author = {{Traefik Labs} and {Palecek, Jacob}},
  year = {2021},
  month = sep,
  abstract = {The Cloud Native Edge Router},
  copyright = {MIT},
  note = {Accessed 2021-11-11},
  howpublished = {\url{https://github.com/jjnp/traefik/tree/load-balancing}}
}


@misc{traefik-dockerhub,
  title = {Traefik | {{Docker Hub}}},
  author = {{Docker Inc.} and {Traefik Labs}},
  note = {Accessed 2021-11-11},
  howpublished = {\url{https://hub.docker.com/_/traefik?tab=tags}}
}


@misc{palecekResponder2021,
  title = {Responder},
  author = {Palecek, Jacob},
  year = {2021},
  month = oct,
  abstract = {Responds to http requests after a set delay. Useful for load testing of network components such as load balancers, and other research topics.},
  copyright = {MIT},
  note = {Accessed 2021-11-11},
  howpublished = {\url{https://github.com/jjnp/responder}}
}


@misc{wrr-kblinux,
  title = {Weighted {{Round}}-{{Robin Scheduling}} - {{LVSKB}}},
  author = {{Linux Virtual Server Authors}},
  note = {Accessed 2021-11-23},
  howpublished = {\url{http://kb.linuxvirtualserver.org/wiki/Weighted_Round-Robin_Scheduling}}
}


@misc{wondernetworkGlobalPingStatistics,
  title = {Global {{Ping Statistics}}},
  author = {{Wonder Network}},
  journal = {WonderNetwork},
  abstract = {WonderNetwork operates a global network of servers and leverages them to provide network testing solutions. View ping times between WonderNetwork servers.},
  note = {Accessed 2021-11-23},
  howpublished = {\url{https://wondernetwork.com/pings}}
}



@inproceedings{beraldiCooperativeLoadBalancing2017,
  title = {Cooperative Load Balancing Scheme for Edge Computing Resources},
  booktitle = {2017 {{Second International Conference}} on {{Fog}} and {{Mobile Edge Computing}} ({{FMEC}})},
  author = {Beraldi, Roberto and Mtibaa, Abderrahmen and Alnuweiri, Hussein},
  year = {2017},
  month = may,
  pages = {94--100},
  publisher = {{IEEE}},
  address = {{Valencia, Spain}},
  doi = {10.1109/FMEC.2017.7946414},
  abstract = {Edge Computing, as a solution to leveraging computation capabilities at the edge of the network, is emerging. One key challenge for edge computing is offering its computing service with a low service blocking and low latency that otherwise translate to an inefficient deployment of a Edge computing system. Unlike cloud computing, the computing resources in edge computing are limited. One way to deal with this limitation is by enabling cooperation between data centers.},
  isbn = {978-1-5386-2859-1},
  langid = {english}
}

@article{gardnerScalableLoadBalancing2021,
  title = {Scalable Load Balancing in the Presence of Heterogeneous Servers},
  author = {Gardner, Kristen and Abdul Jaleel, Jazeem and Wickeham, Alexander and Doroudi, Sherwin},
  year = {2021},
  month = jan,
  journal = {Performance Evaluation},
  volume = {145},
  pages = {102151},
  issn = {01665316},
  doi = {10.1016/j.peva.2020.102151},
  abstract = {Heterogeneity is becoming increasingly ubiquitous in modern large-scale computer systems. Developing good load balancing policies for systems whose resources have varying speeds is crucial in achieving low response times. Indeed, how best to dispatch jobs to servers is a classical and well-studied problem in the queueing literature. Yet the bulk of existing work on large-scale systems assumes homogeneous servers; unfortunately, policies that perform well in the homogeneous setting can cause unacceptably poor performance in heterogeneous systems.},
  langid = {english}
}

@article{karagiannisEdgeRoutingUsingCompute2021,
  title = {{{edgeRouting}}: Using {{Compute Nodes}} in {{Proximity}} to {{Route IoT Data}}},
  shorttitle = {{{edgeRouting}}},
  author = {Karagiannis, Vasileios and Schulte, Stefan},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {105841--105858},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3099942},
  abstract = {Due to the proliferation of edge computing, cloud providers have started offering compute nodes at the edge of the network in addition to traditional compute nodes in data centers. So far, various systems have been proposed for processing Internet of Things (IoT) data on both edge and cloud compute nodes in order to reduce the communication latency. However, such systems do not typically consider that the network bandwidth between an edge node and a cloud node can be orders of magnitude higher than the bandwidth between an IoT device and a cloud node. As a result, the IoT data are commonly sent selectively to either edge or cloud nodes disregarding alternative network paths through edge nodes, which may have higher network bandwidth, and lower communication latency.},
  langid = {english}
}

@inproceedings{kogiasBypassingLoadBalancer2020,
  title = {Bypassing the Load Balancer without Regrets},
  booktitle = {Proceedings of the 11th {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Kogias, Marios and Iyer, Rishabh and Bugnion, Edouard},
  year = {2020},
  month = oct,
  pages = {193--207},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3419111.3421304},
  abstract = {Load balancers are a ubiquitous component of cloud deployments and the cornerstone of workload elasticity. Load balancers can significantly affect the end-to-end application latency with their load balancing decisions, and constitute a significant portion of cloud tenant expenses.},
  isbn = {978-1-4503-8137-6},
  langid = {english}
}

@inproceedings{manjuEfficientLoadBalancing2019,
  title = {Efficient {{Load Balancing Algorithm}} for {{Task Preprocessing}} in {{Fog Computing Environment}}},
  booktitle = {Smart {{Intelligent Computing}} and {{Applications}}},
  author = {Manju, A. B. and Sumathy, S.},
  editor = {Satapathy, Suresh Chandra and Bhateja, Vikrant and Das, Swagatam},
  year = {2019},
  series = {Smart {{Innovation}}, {{Systems}} and {{Technologies}}},
  pages = {291--298},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-1927-3_31},
  abstract = {Focus and research on fog computing environment is increasing in recent days. Orchestrating the resources in the fog computing environment and distributing the task with the help of simple load balancing algorithms improve the task processing in fog environment. Fog resources include end users resources, networking resources, and cloud resources as well, in which networking resources take the central control over the fog nodes at particular location. These control nodes attempt to reduce the burden of cloud as well as improve the task processing efficiency by distributing the load across the fog nodes evenly on controlling the fog nodes based on availability of the nodes. The objective of this work is to evenly distribute the load across the available fog nodes and reduce the response time of the task processing. The results have been verified using cloud analyst tool in which the proposed approach is compared with round-robin algorithm in terms of response time. The results show that the response time has improved substantially in the proposed approach.},
  isbn = {9789811319273},
  langid = {english},
  keywords = {Cloud resources,Fog computing,Fog nodes,Load balancing,Network resources}
}

@article{talaatLoadBalancingOptimization2020,
  title = {A Load Balancing and Optimization Strategy ({{LBOS}}) Using Reinforcement Learning in Fog Computing Environment},
  author = {Talaat, Fatma M. and Saraya, Mohamed S. and Saleh, Ahmed I. and Ali, Hesham A. and Ali, Shereen H.},
  year = {2020},
  month = nov,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {11},
  number = {11},
  pages = {4951--4966},
  issn = {1868-5137, 1868-5145},
  doi = {10.1007/s12652-020-01768-8},
  abstract = {Fog computing (FC) can be considered as a computing paradigm which performs Internet of Things (IoT) applications at the edge of the network. Recently, there is a great growth of data requests and FC which lead to enhance data accessibility and adaptability. However, FC has been exposed to many challenges as load balancing (LB) and adaptation to failure. Many LB strategies have been proposed in cloud computing, but they are still not applied effectively in fog. LB is an important issue to achieve high resource utilization, avoid bottlenecks, avoid overload and low load, and reduce response time. In this paper, a LB and optimization strategy (LBOS) using dynamic resource allocation method based on Reinforcement learning and genetic algorithm is proposed. LBOS monitors the traffic in the network continuously, collects the information about each server load, handles the incoming requests, and distributes them between the available servers equally using dynamic resource allocation method. Hence, it enhances the performance even when it's the peak time. Accordingly, LBOS is simple and efficient in real-time systems in fog computing such as in the case of healthcare system. LBOS is concerned with designing an IoT-Fog based healthcare system. The proposed IoT-Fog system consists of three layers, namely: (1) IoT layer, (2) fog layer, and (3) cloud layer. Finally, the experiments are carried out and the results show that the proposed solution improves the quality-of-service in the cloud/fog computing environment in terms of the allocation cost and reduce the response time. Comparing the LBOS with the state-of-the-art algorithms, it achieved the best load balancing Level (85.71\%). Hence, LBOS is an efficient way to establish the resource utilization and ensure the continuous service.},
  langid = {english}
}

@article{vargaftikLSQLoadBalancing2020,
  title = {{{LSQ}}: Load {{Balancing}} in {{Large}}-{{Scale Heterogeneous Systems With Multiple Dispatchers}}},
  shorttitle = {{{LSQ}}},
  author = {Vargaftik, Shay and Keslassy, Isaac and Orda, Ariel},
  year = {2020},
  month = jun,
  journal = {IEEE/ACM Transactions on Networking},
  volume = {28},
  number = {3},
  pages = {1186--1198},
  issn = {1063-6692, 1558-2566},
  doi = {10.1109/TNET.2020.2980061},
  abstract = {Nowadays, the efficiency and even the feasibility of traditional load-balancing policies are challenged by the rapid growth of cloud infrastructure and the increasing levels of server heterogeneity. In such heterogeneous systems with many loadbalancers, traditional solutions, such as J SQ, incur a prohibitively large communication overhead and detrimental incast effects due to herd behavior. Alternative low-communication policies, such as J SQ(d) and the recently proposed J IQ, are either unstable or provide poor performance. We introduce the Local Shortest Queue (LSQ) family of load balancing algorithms. In these algorithms, each dispatcher maintains its own, local, and possibly outdated view of the server queue lengths, and keeps using J SQ on its local view. A small communication overhead is used infrequently to update this local view. We formally prove that as long as the error in these local estimates of the server queue lengths is bounded in expectation, the entire system is strongly stable. Finally, in simulations, we show how simple and stable LSQ policies exhibit appealing performance and significantly outperform existing low-communication policies, while using an equivalent communication budget. In particular, our simple policies often outperform even J SQ due to their reduction of herd behavior. We further show how, by relying on smart servers (i.e., advanced pull-based communication), we can further improve performance and lower communication overhead.},
  langid = {english}
}

@article{wengOptimalLoadBalancing2020,
  title = {Optimal {{Load Balancing}} with {{Locality Constraints}}},
  author = {Weng, Wentao and Zhou, Xingyu and Srikant, R.},
  year = {2020},
  month = nov,
  journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume = {4},
  number = {3},
  pages = {1--37},
  issn = {2476-1249},
  doi = {10.1145/3428330},
  abstract = {Applications in cloud platforms motivate the study of efficient load balancing under job-server constraints and server heterogeneity. In this paper, we study load balancing on a bipartite graph where left nodes correspond to job types and right nodes correspond to servers, with each edge indicating that a job type can be served by a server. Thus edges represent locality constraints, i.e., an arbitrary job can only be served at servers which contain certain data and/or machine learning (ML) models. Servers in this system can have heterogeneous service rates. In this setting, we investigate the performance of two policies named Join-the-Fastest-of-the-Shortest-Queue (JFSQ) and Join-the-Fastest-of-the-Idle-Queue (JFIQ), which are simple variants of Join-the-Shortest-Queue and Join-the-Idle-Queue, where ties are broken in favor of the fastest servers. Under a "well-connected'' graph condition, we show that JFSQ and JFIQ are asymptotically optimal in the mean response time when the number of servers goes to infinity. In addition to asymptotic optimality, we also obtain upper bounds on the mean response time for finite-size systems. We further show that the well-connectedness condition can be satisfied by a random bipartite graph construction with relatively sparse connectivity.},
  langid = {english}
}

@article{zhangSecureOptimizedLoad2021,
  title = {Secure and {{Optimized Load Balancing}} for {{Multitier IoT}} and {{Edge}}-{{Cloud Computing Systems}}},
  author = {Zhang, Wei-Zhe and Elgendy, Ibrahim A. and Hammad, Mohamed and Iliyasu, Abdullah M. and Du, Xiaojiang and Guizani, Mohsen and {El-Latif}, Ahmed A. Abd},
  year = {2021},
  month = may,
  journal = {IEEE Internet of Things Journal},
  volume = {8},
  number = {10},
  pages = {8119--8132},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2020.3042433},
  langid = {english}
}

@article{zhangStochasticCongestionGame2021,
  title = {Stochastic {{Congestion Game}} for {{Load Balancing}} in {{Mobile}}-{{Edge Computing}}},
  author = {Zhang, Fenghui and Wang, Michael Mao},
  year = {2021},
  month = jan,
  journal = {IEEE Internet of Things Journal},
  volume = {8},
  number = {2},
  pages = {778--790},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2020.3008009},
  abstract = {Mobile-edge computing can reduce task execution delay and improve the Quality of Experience (QoE) for the network edge users. However, when there are multiple independent cloudlets in the network with the mobile users offloading tasks randomly, how to maintain the load balancing of the independent cloudlets, how to improve the quality of service and users' QoE are still issues need to be solved. To this end, we study these issues from the perspective of game theory and propose decentralized learning algorithms. First, we turn the cloudlets load-balancing issue into a competition that each user minimizes its task execution time, and then a stochastic congestion game with incomplete information is proposed. Second, based on the existence proof of the Nash equilibria by using potential game theory, we propose a multiuser decentralized learning algorithm to obtain the pure Nash equilibrium strategy of each user. Then, an ordinary differential equation is derived to prove the convergence of the algorithm. Finally, we propose two application scenarios, one is for static users and the other is for dynamic users, and then the performances of the algorithm in a static scenario is tested. In order to adapt to dynamic scenarios, and further improve the performance and reduce communication costs, we propose a decentralized learning algorithm with termination condition. The experiments show that this algorithm can improve the load balancing of the multicloudlet system, and enhance the quality of service.},
  langid = {english}
}


@inproceedings{aslanpourServerlessEdgeComputing2021,
  title = {Serverless {{Edge Computing}}: Vision and {{Challenges}}},
  shorttitle = {Serverless {{Edge Computing}}},
  booktitle = {2021 {{Australasian Computer Science Week Multiconference}}},
  author = {Aslanpour, Mohammad S. and Toosi, Adel N. and Cicconetti, Claudio and Javadi, Bahman and Sbarski, Peter and Taibi, Davide and Assuncao, Marcos and Gill, Sukhpal Singh and Gaire, Raj and Dustdar, Schahram},
  year = {2021},
  month = feb,
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Dunedin New Zealand}},
  doi = {10.1145/3437378.3444367},
  abstract = {Born from a need for a pure ``pay-per-use'' model and highly scalable platform, the ``Serverless'' paradigm emerged and has the potential to become a dominant way of building cloud applications. Although it was originally designed for cloud environments, Serverless is finding its position in the Edge Computing landscape, aiming to bring computational resources closer to the data source. That is, Serverless is crossing cloud borders to assess its merits in Edge computing, whose principal partner will be the Internet of Things (IoT) applications. This move sounds promising as Serverless brings particular benefits such as eliminating always-on services causing high electricity usage, for instance. However, the community is still hesitant to uptake Serverless Edge Computing because of the cloud-driven design of current Serverless platforms, and distinctive characteristics of edge landscape and IoT applications. In this paper, we evaluate both sides to shed light on the Serverless new territory. Our in-depth analysis promotes a broad vision for bringing Serverless to the Edge Computing. It also issues major challenges for Serverless to be met before entering Edge computing.},
  isbn = {978-1-4503-8956-3},
  langid = {english}
}

@inproceedings{baresiServerlessPlatformEdge2019,
  title = {Towards a {{Serverless Platform}} for {{Edge Computing}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Fog Computing}} ({{ICFC}})},
  author = {Baresi, Luciano and Filgueira Mendonca, Danilo},
  year = {2019},
  month = jun,
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{Prague, Czech Republic}},
  doi = {10.1109/ICFC.2019.00008},
  abstract = {The emergence of real-time and data-intensive applications empowered by mobile computing and IoT devices is challenging the success of centralized data centers, and fostering the adoption of the paradigm of fog/edge computing. Differently from cloud data centers, fog nodes are geographically distributed in proximity to data prosumers, taking advantage of the emerging wireless communication technologies and mobile networks. The limited resources of densely distributed fog nodes call for their efficient use by hosted applications and services. To address this challenge, and the needs of different application scenarios, this paper proposes a serverless platform for edge computing. It starts motivating the adoption of a serverless architecture. Then, it presents the services and mechanisms that are the building blocks of a Serverless Edge Platform. The paper also proposes a prototype platform and its assessment. Obtained results demonstrate the feasibility of the proposed solution for satisfying different application requirements in diverse deployment configurations of heterogeneous fog nodes.},
  isbn = {978-1-72813-236-5},
  langid = {english}
}

@article{cicconettiArchitecturePerformanceEvaluation2020,
  title = {Architecture and Performance Evaluation of Distributed Computation Offloading in Edge Computing},
  author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
  year = {2020},
  month = may,
  journal = {Simulation Modelling Practice and Theory},
  volume = {101},
  pages = {102007},
  issn = {1569190X},
  doi = {10.1016/j.simpat.2019.102007},
  abstract = {Edge computing is an emerging paradigm to enable low-latency applications, like mobile augmented reality, because it takes the computation on processing devices that are closer to the users. On the other hand, the need for highly scalable execution of stateless tasks for cloud systems is driving the definition of new technologies based on serverless computing. In this paper, we propose a novel architecture where the two converge to enable low-latency applications: this is achieved by offloading short-lived stateless tasks from the user terminals to edge nodes. Furthermore, we design a distributed algorithm that tackles the research challenge of selecting the best executor, based on real-time measurements and simple, yet effective, prediction algorithms. Finally, we describe a new performance evaluation framework specifically designed for an accurate assessment of algorithms and protocols in edge computing environments, where the nodes may have very heterogeneous networking and processing capabilities. The proposed framework relies on the use of real components on lightweight virtualization mixed with simulated computation and is well-suited to the analysis of several applications and network environments. Using our framework, we evaluate our proposed architecture and algorithms in small- and large-scale edge computing scenarios, showing that our solution achieves similar or better delay performance than a centralized solution, with far less network utilization.},
  langid = {english}
}

@article{cicconettiDecentralizedFrameworkServerless2020,
  title = {A {{Decentralized Framework}} for {{Serverless Edge Computing}} in the {{Internet}} of {{Things}}},
  author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
  year = {2020},
  journal = {IEEE Transactions on Network and Service Management},
  pages = {1--1},
  issn = {1932-4537, 2373-7379},
  doi = {10.1109/TNSM.2020.3023305},
  abstract = {Serverless computing is becoming widely adopted among cloud providers, thus making increasingly popular the Function-as-a-Service (FaaS) programming model, where the developers realize services by packaging sequences of stateless function calls. The current technologies are very well suited to data centers, but cannot provide equally good performance in decentralized environments, such as edge computing systems, which are expected to be typical for Internet of Things (IoT) applications. In this paper, we fill this gap by proposing a framework for efficient dispatching of stateless tasks to in-network executors so as to minimize the response times while exhibiting short- and long-term fairness, also leveraging information from a virtualized network infrastructure when available. Our solution is shown to be simple enough to be installed on devices with limited computational capabilities, such as IoT gateways, especially when using a hierarchical forwarding extension. We evaluate the proposed platform by means of extensive emulation experiments with a prototype implementation in realistic conditions. The results show that it is able to smoothly adapt to the mobility of clients and to the variations of their service request patterns, while coping promptly with network congestion.},
  langid = {english}
}

@article{cicconettiDistributedComputingEnvironments2020,
  title = {Toward {{Distributed Computing Environments}} with {{Serverless Solutions}} in {{Edge Systems}}},
  author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea and Sabella, Dario},
  year = {2020},
  month = mar,
  journal = {IEEE Communications Magazine},
  volume = {58},
  number = {3},
  pages = {40--46},
  issn = {0163-6804, 1558-1896},
  doi = {10.1109/MCOM.001.1900498},
  abstract = {Computation offloading through stateless applications is gaining momentum thanks to the emergence of serverless frameworks with inherent scalability properties. However, adoption of a serverless framework in an edge computing system requires careful consideration to keep its advantages unscathed. In the cloud, micro-services are scaled automatically according to demands, but in edge computing this would incur a significantly higher cost than in a data center and cannot be as fluid. This is especially relevant in scenarios where edge nodes are spread across large areas and have relatively small computation capabilities. In this article we propose to overcome this issue by adapting the allocation of demands to the currently allocated micro-services at short timescales, with two alternative mechanisms designed for different target scenarios, both aimed at enabling distributed computing environments. The proposed solution can be integrated within the ETSI MEC standard, which specifies a reference architecture and open service interfaces. Our contribution is validated in a proof-of-concept scenario with a prototype implementation released as open source.},
  langid = {english}
}

@inproceedings{gadepalliChallengesOpportunitiesEfficient2019,
  title = {Challenges and {{Opportunities}} for {{Efficient Serverless Computing}} at the {{Edge}}},
  booktitle = {2019 38th {{Symposium}} on {{Reliable Distributed Systems}} ({{SRDS}})},
  author = {Gadepalli, Phani Kishore and Peach, Gregor and Cherkasova, Ludmila and Aitken, Rob and Parmer, Gabriel},
  year = {2019},
  month = oct,
  pages = {261--2615},
  publisher = {{IEEE}},
  address = {{Lyon, France}},
  doi = {10.1109/SRDS47363.2019.00036},
  abstract = {Serverless computing frameworks allow users to execute a small application (dedicated to a specific task) without handling operational issues such as server provisioning, resource management, and resource scaling for the increased load. Serverless computing originally emerged as a Cloud computing framework, but might be a perfect match for IoT data processing at the Edge. However, the existing serverless solutions, based on VMs and containers, are too heavy-weight (large memory footprint and high function invocation time) for operating efficiency and elastic scaling at the Edge. Moreover, many novel IoT applications require low-latency data processing and near real-time responses, which makes the current cloudbased serverless solutions unsuitable. Recently, WebAssembly (Wasm) has been proposed as an alternative method for running serverless applications at near-native speeds, while having a small memory footprint and optimized invocation time. In this paper, we discuss some existing serverless solutions, their design details, and unresolved performance challenges for an efficient serverless management at the Edge. We outline our serverless framework, called aWsm, based on the WebAssembly approach, and discuss the opportunities enabled by the aWsm design, including function profiling and SLO-driven performance management of users' functions. Finally, we present an initial assessment of aWsm performance featuring average startup time (12{$\mu$}s to 30{$\mu$}s) and an economical memory footprint (ranging from 10s to 100s of kB) for a subset of MiBench microbenchmarks used as functions.},
  isbn = {978-1-72814-222-7},
  langid = {english}
}

@inproceedings{gliksonDevicelessEdgeComputing2017,
  title = {Deviceless Edge Computing: Extending Serverless Computing to the Edge of the Network},
  shorttitle = {Deviceless Edge Computing},
  booktitle = {Proceedings of the 10th {{ACM International Systems}} and {{Storage Conference}}},
  author = {Glikson, Alex and Nastic, Stefan and Dustdar, Schahram},
  year = {2017},
  month = may,
  pages = {1--1},
  publisher = {{ACM}},
  address = {{Haifa Israel}},
  doi = {10.1145/3078468.3078497},
  abstract = {The serverless paradigm has been rapidly adopted by developers of cloud-native applications, mainly because it relieves them from the burden of provisioning, scaling and operating the underlying infrastructure. In this paper, we propose a novel computing paradigm \textendash{} Deviceless Edge Computing that extends the serverless paradigm to the edge of the network, enabling IoT and Edge devices to be seamlessly integrated as application execution infrastructure. We also discuss open challenges to realize Deviceless Edge Computing, based on our experience in prototyping a deviceless platform.},
  isbn = {978-1-4503-5035-8},
  langid = {english}
}

@inproceedings{ishakianServingDeepLearning2018,
  title = {Serving {{Deep Learning Models}} in a {{Serverless Platform}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Cloud Engineering}} ({{IC2E}})},
  author = {Ishakian, Vatche and Muthusamy, Vinod and Slominski, Aleksander},
  year = {2018},
  month = apr,
  pages = {257--262},
  publisher = {{IEEE}},
  address = {{Orlando, FL}},
  doi = {10.1109/IC2E.2018.00052},
  abstract = {Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs.},
  isbn = {978-1-5386-5008-0},
  langid = {english}
}

@article{nasticServerlessRealTimeData2017,
  title = {A {{Serverless Real}}-{{Time Data Analytics Platform}} for {{Edge Computing}}},
  author = {Nastic, Stefan and Rausch, Thomas and Scekic, Ognjen and Dustdar, Schahram and Gusev, Marjan and Koteska, Bojana and Kostoska, Magdalena and Jakimovski, Boro and Ristov, Sasko and Prodan, Radu},
  year = {2017},
  journal = {IEEE Internet Computing},
  volume = {21},
  number = {4},
  pages = {64--71},
  issn = {1089-7801},
  doi = {10.1109/MIC.2017.2911430},
  langid = {english}
}

@inproceedings{rauschEdgeIntelligenceConvergence2019,
  title = {Edge {{Intelligence}}: The {{Convergence}} of {{Humans}}, {{Things}}, and {{AI}}},
  shorttitle = {Edge {{Intelligence}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Cloud Engineering}} ({{IC2E}})},
  author = {Rausch, Thomas and Dustdar, Schahram},
  year = {2019},
  month = jun,
  pages = {86--96},
  publisher = {{IEEE}},
  address = {{Prague, Czech Republic}},
  doi = {10.1109/IC2E.2019.00022},
  abstract = {Edge AI and Human Augmentation are two major technology trends, driven by recent advancements in edge computing, IoT, and AI accelerators. As humans, things, and AI continue to grow closer together, systems engineers and researchers are faced with new and unique challenges. In this paper, we analyze the role of edge computing and AI in the cyberhuman evolution, and identify challenges that edge computing systems will consequently be faced with. We take a closer look at how a cyber-physical fabric will be complemented by AI operationalization to enable seamless end-to-end edge intelligence systems.},
  isbn = {978-1-72810-218-4},
  langid = {english}
}

@article{rauschServerlessPlatformEdge,
  title = {Towards a {{Serverless Platform}} for {{Edge AI}}},
  author = {Rausch, Thomas and Hummer, Waldemar and Muthusamy, Vinod and Dustdar, Schahram and Rashed, Alexander},
  pages = {7},
  abstract = {This paper proposes a serverless platform for building and operating edge AI applications. We analyze edge AI use cases to illustrate the challenges in building and operating AI applications in edge cloud scenarios. By elevating concepts from AI lifecycle management into the established serverless model, we enable easy development of edge AI workflow functions. We take a deviceless approach, i.e., we treat edge resources transparently as cluster resources, but give developers fine-grained control over scheduling constraints. Furthermore, we demonstrate the limitations of current serverless function schedulers, and present the current state of our prototype.},
  langid = {english},
  keywords = {used},
  booktitle = {2nd {USENIX} Workshop on Hot Topics in Edge Computing (HotEdge 19)},
    year = {2019},
    address = {Renton, WA},
    url = {https://www.usenix.org/conference/hotedge19/presentation/rausch},
    publisher = {{USENIX} Association},
    month = jul
}


@article{bermbachAuctionBasedFunctionPlacement2020,
  title = {Towards {{Auction}}-{{Based Function Placement}} in {{Serverless Fog Platforms}}},
  author = {Bermbach, David and Maghsudi, Setareh and Hasenburg, Jonathan and Pfandzelter, Tobias},
  year = {2020},
  month = mar,
  journal = {arXiv:1912.06096 [cs]},
  eprint = {1912.06096},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The Function-as-a-Service (FaaS) paradigm has a lot of potential as a computing model for fog environments comprising both cloud and edge nodes. When the request rate exceeds capacity limits at the edge, some functions need to be offloaded from the edge towards the cloud.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Networking and Internet Architecture},
  note = {http://arxiv.org/abs/1912.06096}
}

@inproceedings{gaoWinningStartingLine2019,
  title = {Winning at the {{Starting Line}}: Joint {{Network Selection}} and {{Service Placement}} for {{Mobile Edge Computing}}},
  shorttitle = {Winning at the {{Starting Line}}},
  booktitle = {{{IEEE INFOCOM}} 2019 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Gao, Bin and Zhou, Zhi and Liu, Fangming and Xu, Fei},
  year = {2019},
  month = apr,
  pages = {1459--1467},
  publisher = {{IEEE}},
  address = {{Paris, France}},
  doi = {10.1109/INFOCOM.2019.8737543},
  abstract = {Mobile Edge Computing (MEC) is an emerging computing paradigm in which computational capabilities are pushed from the central cloud to the network edges. However, preserving the satisfactory quality-of-service (QoS) for user applications is non-trivial among multiple densely dispersed yet capacity constrained MEC nodes. This is mainly because both the access network and edge nodes are vulnerable to network congestion. Previous works are mostly limited to optimizing the QoS through dynamic service placement, while ignoring the critical effects of access network selection on the network congestion. In this paper, we study the problem of jointly optimizing the access network selection and service placement for MEC, towards the goal of improving the QoS by balancing the access, switching and communication delay. Specifically, we first design an efficient online framework to decompose the long-term optimization problem into a series of one-shot problems. To address the NP-hardness of the one-shot problem, we further propose an iteration-based algorithm to derive a computation efficient solution. Both rigorous theoretical analysis on the optimality gap and extensive trace-driven simulations validate the efficacy of our proposed solution.},
  isbn = {978-1-72810-515-4},
  langid = {english}
}

@article{maContainerMigrationMechanism2020,
  title = {Container {{Migration Mechanism}} for {{Load Balancing}} in {{Edge Network Under Power Internet}} of {{Things}}},
  author = {Ma, Zitong and Shao, Sujie and Guo, Shaoyong and Wang, Zhili and Qi, Feng and Xiong, Ao},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {118405--118416},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3004615},
  abstract = {As a novel computing technology closer to business ends, edge computing has become an effective solution for delay sensitive business of power Internet of Things (IoT). However, the uneven spatial and temporal distribution of business requests in edge network leads to a significant difference in business busyness between edge nodes. Due to the natural lightweight and portability, container migration has become a critical technology for load balancing, thereby optimizing the resource utilization of edge nodes. To this end, this paper proposes a container migration-based decision-making (CMDM) mechanism in power IoT. First, a load differentiation matrix model between edge nodes is constructed to determine the timing of container migration. Then, a container migration model of load balancing joint migration cost (LBJC) is established to minimize the impact of container migration while balancing the load of edge network. Finally, the migration priority of containers is determined from the perspective of resource correlation and business relevance, and by introducing a pseudo-random ratio rule and combining the local pheromone evaporation with global pheromone update at the same time, a migration algorithm based on modified Ant Colony System (MACS) is designed to utilize the LBJC model and thus guiding the choice of possible migration actions. The simulation results show that compared with genetic algorithm (GA) and Space Aware Best Fit Decreasing (SABFD) algorithm, the comprehensive performance of CMDM in load balancing joint migration cost is improved by 7.3\% and 12.5\% respectively.},
  langid = {english}
}

@article{nezamiDecentralizedEdgetoCloudLoad2021,
  title = {Decentralized {{Edge}}-to-{{Cloud Load Balancing}}: Service {{Placement}} for the {{Internet}} of {{Things}}},
  shorttitle = {Decentralized {{Edge}}-to-{{Cloud Load Balancing}}},
  author = {Nezami, Zeinab and Zamanifar, Kamran and Djemame, Karim and Pournaras, Evangelos},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {64983--65000},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3074962},
  abstract = {The Internet of Things (IoT) requires a new processing paradigm that inherits the scalability of the cloud while minimizing network latency using resources closer to the network edge. On the one hand, building up such flexibility within the edge-to-cloud continuum consisting of a distributed networked ecosystem of heterogeneous computing resources is challenging. On the other hand, IoT traffic dynamics and the rising demand for low-latency services foster the need for minimizing the response time and a balanced service placement. Load-balancing for fog computing becomes a cornerstone for cost-effective system management and operations. This paper studies two optimization objectives and formulates a decentralized load-balancing problem for IoT service placement: (global) IoT workload balance and (local) quality of service (QoS), in terms of minimizing the cost of deadline violation, service deployment, and unhosted services. The proposed solution, EPOS Fog, introduces a decentralized multi-agent system for collective learning that utilizes edge-to-cloud nodes to jointly balance the input workload across the network and minimize the costs involved in service execution. The agents locally generate possible assignments of requests to resources and then cooperatively select an assignment such that their combination maximizes edge utilization while minimizes service execution cost. Extensive experimental evaluation with realistic Google cluster workloads on various networks demonstrates the superior performance of EPOS Fog in terms of workload balance and QoS, compared to approaches such as First Fit and exclusively Cloud-based. The results confirm that EPOS Fog reduces service execution delay up to 25\% and the load-balance of network nodes up to 90\%. The findings also demonstrate how distributed computational resources on the edge can be utilized more cost-effectively by harvesting collective intelligence.},
  langid = {english}
}

@article{yangEdgeCoordinatedQuery2020,
  title = {Edge {{Coordinated Query Configuration}} for {{Low}}-{{Latency}} and {{Accurate Video Analytics}}},
  author = {Yang, Peng and Lyu, Feng and Wu, Wen and Zhang, Ning and Yu, Li and Shen, Xuemin Sherman},
  year = {2020},
  month = jul,
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {16},
  number = {7},
  pages = {4855--4864},
  issn = {1551-3203, 1941-0050},
  doi = {10.1109/TII.2019.2949347},
  abstract = {To develop smart city and intelligent manufacturing, video cameras are being increasingly deployed. In order to achieve fast and accurate response to live video queries (e.g., license plate recording and object tracking), the real-time highvolume video streams should be delivered and analyzed efficiently. In this paper, we introduce an end-edge-cloud coordination framework for low-latency and accurate live video analytics. Considering the locality of video queries, edge platform is designated as the system coordinator. It accepts live video queries and configures the related end cameras to generate video frames that meet quality requirements. By taking into account the latency constraint, edge computing resources are subtly distributed to process the live video frames from different sources, such that the analytic accuracy of the accepted video queries can be maximized. Since the amount of required edge computing resource and video quality to accurately address different video queries are unknown in advance, we propose an online video quality and computing resource configuration algorithm to gradually learn the optimal configuration strategy. Extensive simulation results show that, as compared to other benchmarks, the proposed configuration algorithm can effectively improve the analytic accuracy, while providing low-latency response.},
  langid = {english}
}

@inproceedings{zhaoOptimalPlacementVirtual2017,
  title = {Optimal {{Placement}} of {{Virtual Machines}} in {{Mobile Edge Computing}}},
  booktitle = {{{GLOBECOM}} 2017 - 2017 {{IEEE Global Communications Conference}}},
  author = {Zhao, Lei and Liu, Jiajia and Shi, Yongpeng and Sun, Wen and Guo, Hongzhi},
  year = {2017},
  month = dec,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/GLOCOM.2017.8254084},
  abstract = {Mobile edge computing (MEC), as an extension of the cloud computing paradigm to the edge network, is a promising solution to provide resource-intensive and timecritical applications to mobile users. It overcomes some obstacles of traditional mobile cloud computing by offering ultra-short latency and less core network traffic. This paper proposes a new framework based on the architecture of MEC to deliver cloud services to the edge. We introduce enumeration based optimal placement algorithm (EOPA) and divide-and-conquer based nearoptimal placement algorithm (DCNOPA) to attain minimal data traffic by distributing virtual machine replica copies (VRCs) of applications to the edge network. Simulation results show that compared to the famous K-medians clustering algorithm (KMCA), the performance of DCNOPA is much closer to that of EOPA with lower computational complexity. Furthermore, we investigate the optimal number of VRCs within a given limitation of benefit-to-cost ratio.},
  isbn = {978-1-5090-5019-2},
  langid = {english}
}

@article{zhaoOptimalPlacementVirtual2018,
  title = {Optimal {{Placement}} of {{Virtual Machines}} for {{Supporting Multiple Applications}} in {{Mobile Edge Networks}}},
  author = {Zhao, Lei and Liu, Jiajia},
  year = {2018},
  journal = {IEEE Transactions on Vehicular Technology},
  pages = {1--1},
  issn = {0018-9545, 1939-9359},
  doi = {10.1109/TVT.2018.2808171},
  abstract = {Although mobile edge computing (MEC), as an extension of the cloud computing paradigm to edge networks, overcomes some obstacles of traditional mobile cloud computing, i.e., the reduced response time particularly, it is a nontrivial task to efficiently deploy virtual machine replica copies (VRCs) supporting multiple applications among numerous MEC servers in edge networks. To combat this issue, we are motivated to investigate in detail the optimal placement of VRCs to minimize the average response time (MART) in the MEC architecture with various requests demand among multiple applications and capacity constraints of MEC servers in edge networks. Besides optimal enumeration placement algorithm (OEPA) as benchmark, we design latency aware heuristic placement algorithm (LAHPA) with much lower computation complexity than that of OEPA. To enhance the performance of LAHPA on MART, clustering enhanced heuristic placement algorithm (CEHPA) is proposed, focusing on the optimal VRC placement in each cluster. We also develop substitution enhanced heuristic placement algorithm (SEHPA) to avoid falling into local optimal solutions. As corroborated by extensive simulation results, the performance of SEHPA on MART is very close to that of OEPA compared with LAHPA and CEHPA. Note that CEHPA also outperforms LAHPA, and both are better than a general greedy placement algorithm. Furthermore, we evaluate the normalized total cost for services provision in edge networks, where SEHPA can also get more outstanding results than other algorithms.},
  langid = {english}
}


@inproceedings{akkusSANDHighPerformanceServerless2018,
  title = {\{\vphantom\}{{SAND}}\vphantom\{\}: Towards {{High}}-{{Performance Serverless Computing}}},
  shorttitle = {\{\vphantom\}{{SAND}}\vphantom\{\}},
  booktitle = {2018 \{\vphantom\}{{USENIX}}\vphantom\{\} {{Annual Technical Conference}} (\{\vphantom\}{{USENIX}}\vphantom\{\} \{\vphantom\}{{ATC}}\vphantom\{\} 18)},
  author = {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya, Paarijaat and Hilt, Volker},
  year = {2018},
  pages = {923--935},
  isbn = {978-1-939133-01-4},
  langid = {english},
  note = {https://www.usenix.org/conference/atc18/presentation/akkus}
}

@article{castroServerDeadLong2019,
  title = {The Server Is Dead, Long Live the Server: Rise of {{Serverless Computing}}, {{Overview}} of {{Current State}} and {{Future Trends}} in {{Research}} and {{Industry}}},
  shorttitle = {The Server Is Dead, Long Live the Server},
  author = {Castro, Paul and Ishakian, Vatche and Muthusamy, Vinod and Slominski, Aleksander},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.02888 [cs]},
  eprint = {1906.02888},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Serverless computing -- an emerging cloud-native paradigm for the deployment of applications and services -- represents an evolution in cloud application development, programming models, abstractions, and platforms. It promises a real pay-as-you-go billing (with millisecond granularity) with no waste of resources, and lowers the bar for developers by asking them to delegate all their operational complexity and scalability to the cloud provider. Delivering on these promises comes at the expense of restricting functionality. In this article we provide an overview of serverless computing, its evolution, general architecture, key characteristics and uses cases that made it an attractive option for application development. Based on discussions with academics and industry experts during a series of organized serverless computing workshops (WoSC), we also identify the technical challenges and open problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Software Engineering},
  note = {http://arxiv.org/abs/1906.02888}
}

@article{jonasCloudProgrammingSimplified2019,
  title = {Cloud {{Programming Simplified}}: A {{Berkeley View}} on {{Serverless Computing}}},
  shorttitle = {Cloud {{Programming Simplified}}},
  author = {Jonas, Eric and {Schleier-Smith}, Johann and Sreekanti, Vikram and Tsai, Chia-Che and Khandelwal, Anurag and Pu, Qifan and Shankar, Vaishaal and Carreira, Joao and Krauth, Karl and Yadwadkar, Neeraja and Gonzalez, Joseph E. and Popa, Raluca Ada and Stoica, Ion and Patterson, David A.},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.03383 [cs]},
  eprint = {1902.03383},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Operating Systems},
  note = {http://arxiv.org/abs/1902.03383}
}

@inproceedings{khandelwalTaureauDeconstructingServerless2020,
  title = {Le {{Taureau}}: Deconstructing the {{Serverless Landscape}} \&amp; {{A Look Forward}}},
  shorttitle = {Le {{Taureau}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Khandelwal, Anurag and Kejariwal, Arun and Ramasamy, Karthikeyan},
  year = {2020},
  month = jun,
  series = {{{SIGMOD}} '20},
  pages = {2641--2650},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3318464.3383130},
  abstract = {Akin to the natural evolution of programming in assembly language to high-level languages, serverless computing represents the next frontier in the evolution of cloud computing: bare metal -{$>$} virtual machines -{$>$} containers -{$>$} serverless. The genesis of serverless computing can be traced back to the fundamental need of enabling a programmer to singularly focus on writing application code in a high-level language and isolating all facets of system management (for example, but not limited to, instance selection, scaling, deployment, logging, monitoring, fault tolerance and so on). This is particularly critical in light of today's, increasingly tightening, time-to-market constraints. Currently, serverless computing is supported by leading public cloud vendors, such as AWS Lambda, Google Cloud Functions, Azure Cloud Functions and others. While this is an important step in the right direction, there are many challenges going forward. For instance, but not limited to, how to enable support for dynamic optimization, how to extend support for stateful computation, how to efficiently bin-pack applications, how to support hardware heterogeneity (this will be key especially in light of the emergence of hardware accelerators for deep learning workloads). Inspired by Picasso's Le Taureau, in the tutorial proposed herein, we shall deconstruct evolution of serverless --- the overarching intent being to facilitate better understanding of the serverless landscape. This, we hope, would help push the innovation frontier on both fronts, the paradigm itself and the applications built atop of it.},
  isbn = {978-1-4503-6735-6},
  keywords = {cloud computing,data analytics,distributed systems,ephemeral storage,real-time streaming,serverless computing}
}

@inproceedings{mcgrathServerlessComputingDesign2017,
  title = {Serverless {{Computing}}: Design, {{Implementation}}, and {{Performance}}},
  shorttitle = {Serverless {{Computing}}},
  booktitle = {2017 {{IEEE}} 37th {{International Conference}} on {{Distributed Computing Systems Workshops}} ({{ICDCSW}})},
  author = {McGrath, Garrett and Brenner, Paul R.},
  year = {2017},
  month = jun,
  pages = {405--410},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1109/ICDCSW.2017.36},
  abstract = {We present the design of a novel performanceoriented serverless computing platform implemented in .NET, deployed in Microsoft Azure, and utilizing Windows containers as function execution environments. Implementation challenges such as function scaling and container discovery, lifecycle, and reuse are discussed in detail. We propose metrics to evaluate the execution performance of serverless platforms and conduct tests on our prototype as well as AWS Lambda, Azure Functions, Google Cloud Functions, and IBM's deployment of Apache OpenWhisk. Our measurements show the prototype achieving greater throughput than other platforms at most concurrency levels, and we examine the scaling and instance expiration trends in the implementations. Additionally, we discuss the gaps and limitations in our current design, propose possible solutions, and highlight future research.},
  isbn = {978-1-5386-3292-5},
  langid = {english}
}

@inproceedings{mohantyEvaluationOpenSource2018,
  title = {An {{Evaluation}} of {{Open Source Serverless Computing Frameworks}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Cloud Computing Technology}} and {{Science}} ({{CloudCom}})},
  author = {Mohanty, Sunil Kumar and Premsankar, Gopika and {di Francesco}, Mario},
  year = {2018},
  month = dec,
  pages = {115--120},
  publisher = {{IEEE}},
  address = {{Nicosia}},
  doi = {10.1109/CloudCom2018.2018.00033},
  abstract = {Recent advancements in virtualization and software architecture have led to the new paradigm of serverless computing, which allows developers to deploy applications as stateless functions without worrying about the underlying infrastructure. Accordingly, a serverless platform handles the lifecycle, execution and scaling of the actual functions; these need to run only when invoked or triggered by an event. Thus, the major benefits of serverless computing are low operational concerns and efficient resource management and utilization. Serverless computing is currently offered by several public cloud service providers. However, there are certain limitations on the public cloud platforms, such as vendor lock-in and restrictions on the computation of the functions. Open source serverless frameworks are a promising solution to avoid these limitations and bring the power of serverless computing to on-premise deployments. However, these frameworks have not been evaluated before. Thus, we carry out a comprehensive feature comparison of popular open source serverless computing frameworks. We then evaluate the performance of selected frameworks: Fission, Kubeless and OpenFaaS. Specifically, we characterize the response time and ratio of successfully received responses under different loads and provide insights into the design choices of each framework.},
  isbn = {978-1-5386-7899-2},
  langid = {english}
}

@inproceedings{wangPeekingCurtainsServerless2018,
  title = {Peeking {{Behind}} the {{Curtains}} of {{Serverless Platforms}}},
  booktitle = {2018 \{\vphantom\}{{USENIX}}\vphantom\{\} {{Annual Technical Conference}} (\{\vphantom\}{{USENIX}}\vphantom\{\} \{\vphantom\}{{ATC}}\vphantom\{\} 18)},
  author = {Wang, Liang and Li, Mengyuan and Zhang, Yinqian and Ristenpart, Thomas and Swift, Michael},
  year = {2018},
  pages = {133--146},
  isbn = {978-1-939133-01-4},
  langid = {english},
  note = {https://www.usenix.org/conference/atc18/presentation/wang-liang}
}


@misc{kubernetes-hpa,
  title = {Horizontal {{Pod Autoscaler}}},
  author = {{Kubernetes Authors} and {The Linux Foundation}},
  journal = {Kubernetes},
  abstract = {The Horizontal Pod Autoscaler automatically scales the number of Pods in a replication controller, deployment, replica set or stateful set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics). Note that Horizontal Pod Autoscaling does not apply to objects that can't be scaled, for example, DaemonSets. The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller.},
  chapter = {docs},
  langid = {english},
  note = {Accessed 2021-11-30},
  howpublished = {\url{https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/}}
}

@misc{openfaas-autoscaling,
  title = {Autoscaling - {{OpenFaaS}}},
  author = {{OpenFaaS Authors}},
  note = {Accessed 2021-11-30},
  howpublished = {\url{https://docs.openfaas.com/architecture/autoscaling/}}
}



@article{abbasMobileEdgeComputing2018b,
  title = {Mobile {{Edge Computing}}: A {{Survey}}},
  shorttitle = {Mobile {{Edge Computing}}},
  author = {Abbas, Nasir and Zhang, Yan and Taherkordi, Amir and Skeie, Tor},
  year = {2018},
  month = feb,
  journal = {IEEE Internet of Things Journal},
  volume = {5},
  number = {1},
  pages = {450--465},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2017.2750180},
  abstract = {Mobile edge computing (MEC) is an emergent architecture where cloud computing services are extended to the edge of networks leveraging mobile base stations. As a promising edge technology, it can be applied to mobile, wireless, and wireline scenarios, using software and hardware platforms, located at the network edge in the vicinity of end-users. MEC provides seamless integration of multiple application service providers and vendors toward mobile subscribers, enterprises, and other vertical segments. It is an important component in the 5G architecture which supports variety of innovative applications and services where ultralow latency is required. This paper is aimed to present a comprehensive survey of relevant research and technological developments in the area of MEC. It provides the definition of MEC, its advantages, architectures, and application areas; where we in particular highlight related research and future directions. Finally, security and privacy issues and related existing solutions are also discussed.},
  keywords = {Cloud computing,Edge computing,Fog computing,Internet of Things,Internet of Things (IoT),mobile cloud computing (MCC),Mobile communication,Mobile computing,mobile edge computing (MEC),Radio access networks,Servers}
}

@inproceedings{paladeEvaluationOpenSource2019,
  title = {An {{Evaluation}} of {{Open Source Serverless Computing Frameworks Support}} at the {{Edge}}},
  booktitle = {2019 {{IEEE World Congress}} on {{Services}} ({{SERVICES}})},
  author = {Palade, Andrei and Kazmi, Aqeel and Clarke, Siobhan},
  year = {2019},
  month = jul,
  pages = {206--211},
  publisher = {{IEEE}},
  address = {{Milan, Italy}},
  doi = {10.1109/SERVICES.2019.00057},
  abstract = {The proliferation of Internet of Things (IoT) and the success of resource-rich cloud services have pushed the data processing horizon towards the edge of the network. This has the potential to address bandwidth costs, and latency, availability and data privacy concerns. Serverless computing, a cloud computing model for stateless and event-driven applications, promises to further improve Quality of Service (QoS) by eliminating the burden of always-on infrastructure through ephemeral containers. Open source serverless frameworks have been introduced to avoid the vendor lock-in and computation restrictions of public cloud platforms and to bring the power of serverless computing to onpremises deployments. In an IoT environment, these frameworks can leverage the computational capabilities of devices in the local network to further improve QoS of applications delivered to the user. However, these frameworks have not been evaluated in a resource-constrained, edge computing environment. In this work we evaluate four open source serverless frameworks, namely, Kubeless, Apache OpenWhisk, OpenFaaS, Knative. Each framework is installed on a bare-metal, single master, Kubernetes cluster. We use the JMeter framework to evaluate the response time, throughput and success rate of functions deployed using these frameworks under different workloads. The evaluation results are presented and open research opportunities are discussed.},
  isbn = {978-1-72813-851-0},
  langid = {english}
}

@article{satyanarayananEmergenceEdgeComputing2017,
  title = {The {{Emergence}} of {{Edge Computing}}},
  author = {Satyanarayanan, Mahadev},
  year = {2017},
  month = jan,
  journal = {Computer},
  volume = {50},
  number = {1},
  pages = {30--39},
  issn = {1558-0814},
  doi = {10.1109/MC.2017.9},
  abstract = {Industry investment and research interest in edge computing, in which computing and storage nodes are placed at the Internet's edge in close proximity to mobile devices or sensors, have grown dramatically in recent years. This emerging technology promises to deliver highly responsive cloud services for mobile computing, scalability and privacy-policy enforcement for the Internet of Things, and the ability to mask transient cloud outages. The web extra at www.youtube.com/playlist?list=PLmrZVvFtthdP3fwHPy\_4d61oDvQY\_RBgS includes a five-video playlist demonstrating proof-of-concept implementations for three tasks: assembling 2D Lego models, freehand sketching, and playing Ping-Pong.},
  keywords = {AR,augmented reality,Augmented reality,cloud,Cloud computing,cloudlets,cognitive assistance,computer vision,Computer vision,content delivery networks,Content management,Data analysis,data analytics,edge computing,Edge computing,fog computing,GigaSight,Internet of things,Internet of Things,Investments,IoT,mobile,networking,Outlook,pervasive computing,privacy,security,telecommunications,virtual reality,VR}
}

@article{shiPromiseEdgeComputing2016,
  title = {The {{Promise}} of {{Edge Computing}}},
  author = {Shi, Weisong and Dustdar, Schahram},
  year = {2016},
  month = may,
  journal = {Computer},
  volume = {49},
  number = {5},
  pages = {78--81},
  issn = {0018-9162},
  doi = {10.1109/MC.2016.145},
  langid = {english}
}



@article{chinchaliNetworkOffloadingPolicies2021,
  title = {Network Offloading Policies for Cloud Robotics: A Learning-Based Approach},
  shorttitle = {Network Offloading Policies for Cloud Robotics},
  author = {Chinchali, Sandeep and Sharma, Apoorva and Harrison, James and Elhafsi, Amine and Kang, Daniel and Pergament, Evgenya and Cidon, Eyal and Katti, Sachin and Pavone, Marco},
  year = {2021},
  month = oct,
  journal = {Autonomous Robots},
  volume = {45},
  number = {7},
  pages = {997--1012},
  issn = {1573-7527},
  doi = {10.1007/s10514-021-09987-4},
  abstract = {Today's robotic systems are increasingly turning to computationally expensive models such as deep neural networks (DNNs) for tasks like localization, perception, planning, and object detection. However, resource-constrained robots, like low-power drones, often have insufficient on-board compute resources or power reserves to scalably run the most accurate, state-of-the art neural network compute models. Cloud robotics allows mobile robots the benefit of offloading compute to centralized servers if they are uncertain locally or want to run more accurate, compute-intensive models. However, cloud robotics comes with a key, often understated cost: communicating with the cloud over congested wireless networks may result in latency or loss of data. In fact, sending high data-rate video or LIDAR from multiple robots over congested networks can lead to prohibitive delay for real-time applications, which we measure experimentally. In this paper, we formulate a novel Robot Offloading Problem\textemdash how and when should robots offload sensing tasks, especially if they are uncertain, to improve accuracy while minimizing the cost of cloud communication? We formulate offloading as a sequential decision making problem for robots, and propose a solution using deep reinforcement learning. In both simulations and hardware experiments using state-of-the art vision DNNs, our offloading strategy improves vision task performance by between 1.3 and 2.3\$\$\textbackslash times \$\$of benchmark offloading strategies, allowing robots the potential to significantly transcend their on-board sensing accuracy but with limited cost of cloud communication.},
  langid = {english}
}

@inproceedings{haWearableCognitiveAssistance2014,
  title = {Towards Wearable Cognitive Assistance},
  booktitle = {Proceedings of the 12th Annual International Conference on {{Mobile}} Systems, Applications, and Services},
  author = {Ha, Kiryong and Chen, Zhuo and Hu, Wenlu and Richter, Wolfgang and Pillai, Padmanabhan and Satyanarayanan, Mahadev},
  year = {2014},
  month = jun,
  series = {{{MobiSys}} '14},
  pages = {68--81},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2594368.2594383},
  abstract = {We describe the architecture and prototype implementation of an assistive system based on Google Glass devices for users in cognitive decline. It combines the first-person image capture and sensing capabilities of Glass with remote processing to perform real-time scene interpretation. The system architecture is multi-tiered. It offers tight end-to-end latency bounds on compute-intensive operations, while addressing concerns such as limited battery capacity and limited processing capability of wearable devices. The system gracefully degrades services in the face of network failures and unavailability of distant architectural tiers.},
  isbn = {978-1-4503-2793-0},
  keywords = {cloud computing,cloud offload,cloudlet,cyber foraging,google glass,mobile computing,virtual machine,wearable computing}
}

@article{liEdgeAIOnDemand2020,
  title = {Edge {{AI}}: On-{{Demand Accelerating Deep Neural Network Inference}} via {{Edge Computing}}},
  shorttitle = {Edge {{AI}}},
  author = {Li, En and Zeng, Liekang and Zhou, Zhi and Chen, Xu},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Wireless Communications},
  volume = {19},
  number = {1},
  pages = {447--457},
  issn = {1558-2248},
  doi = {10.1109/TWC.2019.2946140},
  abstract = {As a key technology of enabling Artificial Intelligence (AI) applications in 5G era, Deep Neural Networks (DNNs) have quickly attracted widespread attention. However, it is challenging to run computation-intensive DNN-based tasks on mobile devices due to the limited computation resources. What's worse, traditional cloud-assisted DNN inference is heavily hindered by the significant wide-area network latency, leading to poor real-time performance as well as low quality of user experience. To address these challenges, in this paper, we propose Edgent, a framework that leverages edge computing for DNN collaborative inference through device-edge synergy. Edgent exploits two design knobs: (1) DNN partitioning that adaptively partitions computation between device and edge for purpose of coordinating the powerful cloud resource and the proximal edge resource for real-time DNN inference; (2) DNN right-sizing that further reduces computing latency via early exiting inference at an appropriate intermediate DNN layer. In addition, considering the potential network fluctuation in real-world deployment, Edgent is properly design to specialize for both static and dynamic network environment. Specifically, in a static environment where the bandwidth changes slowly, Edgent derives the best configurations with the assist of regression-based prediction models, while in a dynamic environment where the bandwidth varies dramatically, Edgent generates the best execution plan through the online change point detection algorithm that maps the current bandwidth state to the optimal configuration. We implement Edgent prototype based on the Raspberry Pi and the desktop PC and the extensive experimental evaluations demonstrate Edgent's effectiveness in enabling on-demand low-latency edge intelligence.},
  keywords = {Bandwidth,computation offloading,Computational modeling,deep learning,edge computing,Edge computing,Edge intelligence,Image edge detection,Mobile handsets,Performance evaluation,Wireless communication}
}

@article{wangWearMaskFastInbrowser2021,
  title = {{{WearMask}}: Fast {{In}}-Browser {{Face Mask Detection}} with {{Serverless Edge Computing}} for {{COVID}}-19},
  shorttitle = {{{WearMask}}},
  author = {Wang, Zekun and Wang, Pengwei and Louis, Peter C. and Wheless, Lee E. and Huo, Yuankai},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.00784 [cs, eess]},
  eprint = {2101.00784},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {The COVID-19 epidemic has been a significant healthcare challenge in the United States. According to the Centers for Disease Control and Prevention (CDC), COVID-19 infection is transmitted predominately by respiratory droplets generated when people breathe, talk, cough, or sneeze. Wearing a mask is the primary, effective, and convenient method of blocking 80\% of all respiratory infections. Therefore, many face mask detection and monitoring systems have been developed to provide effective supervision for hospitals, airports, publication transportation, sports venues, and retail locations. However, the current commercial face mask detection systems are typically bundled with specific software or hardware, impeding public accessibility. In this paper, we propose an in-browser serverless edge-computing based face mask detection solution, called Webbased efficient AI recognition of masks (WearMask), which can be deployed on any common devices (e.g., cell phones, tablets, computers) that have internet connections using web browsers, without installing any software. The serverless edge-computing design minimizes the extra hardware costs (e.g., specific devices or cloud computing servers). The contribution of the proposed method is to provide a holistic edge-computing framework of integrating (1) deep learning models (YOLO), (2) high-performance neural network inference computing framework (NCNN), and (3) a stack-based virtual machine (WebAssembly). For end-users, our web-based solution has advantages of (1) serverless edgecomputing design with minimal device limitation and privacy risk, (2) installation free deployment, (3) low computing requirements, and (4) high detection speed. Our WearMask application has been launched with public access at facemask-detection.com.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  note = {http://arxiv.org/abs/2101.00784}
}

@article{zhangEdgeVideoAnalytics2019,
  title = {Edge {{Video Analytics}} for {{Public Safety}}: A {{Review}}},
  shorttitle = {Edge {{Video Analytics}} for {{Public Safety}}},
  author = {Zhang, Qingyang and Sun, Hui and Wu, Xiaopei and Zhong, Hong},
  year = {2019},
  month = aug,
  journal = {Proceedings of the IEEE},
  volume = {107},
  number = {8},
  pages = {1675--1696},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2019.2925910},
  langid = {english}
}



@article{bryhniComparisonLoadBalancing2000,
  title = {A Comparison of Load Balancing Techniques for Scalable {{Web}} Servers},
  author = {Bryhni, H. and Klovning, E. and Kure, O.},
  year = {2000},
  month = jul,
  journal = {IEEE Network},
  volume = {14},
  number = {4},
  pages = {58--64},
  issn = {1558-156X},
  doi = {10.1109/65.855480},
  abstract = {Scalable Web servers can be built using a network of workstations where server capacity can be extended by adding new workstations as the workload increases. The topic of our article is a comparison of different method to do load-balancing of HTTP traffic for scalable Web servers. We present a classification framework the different load-balancing methods and compare their performance. In addition, we evaluate in detail one class of methods using a prototype implementation with instruction-level analysis of processing overhead. The comparison is based on a trace driven simulation of traces from a large ISP (Internet Service Provider) in Norway. The simulation model is used to analyze different load-balancing schemes based on redirection of request in the network and redirection in the mapping between a canonical name (CNAME) and IP address. The latter is vulnerable to spatial and temporal locality, although for the set of traces used, the impact of locality is limited. The best performance is obtained with redirection in the network.},
  keywords = {Internet,Load management,Network servers,Protocols,Prototypes,Research and development,Telecommunication traffic,Traffic control,Web server,Workstations}
}

@article{cardelliniDynamicLoadBalancing1999a,
  title = {Dynamic Load Balancing on {{Web}}-Server Systems},
  author = {Cardellini, V. and Colajanni, M. and Yu, P.S.},
  year = {1999},
  month = may,
  journal = {IEEE Internet Computing},
  volume = {3},
  number = {3},
  pages = {28--39},
  issn = {1941-0131},
  doi = {10.1109/4236.769420},
  abstract = {Popular Web sites cannot rely on a single powerful server nor on independent mirrored-servers to support the ever-increasing request load. Distributed Web server architectures that transparently schedule client requests offer a way to meet dynamic scalability and availability requirements. The authors review the state of the art in load balancing techniques on distributed Web-server systems, and analyze the efficiencies and limitations of the various approaches.},
  keywords = {Computer architecture,Internet,Load management,NASA,Network servers,Scalability,Scheduling,Uniform resource locators,Web server,Web sites}
}



@article{dayOSIReferenceModel1983,
  title = {The {{OSI}} Reference Model},
  author = {Day, J.D. and Zimmermann, H.},
  year = {1983},
  month = dec,
  journal = {Proceedings of the IEEE},
  volume = {71},
  number = {12},
  pages = {1334--1340},
  issn = {1558-2256},
  doi = {10.1109/PROC.1983.12775},
  abstract = {The early successes of computer networks in the mid-1970's made it apparent that to utilize the full potential of computer networks, international standards would be required. In 1977, the International Standards Organization (ISO) initiated work on Open Systems Interconnection (OSI) to address these requirements. This paper briefly describes the OSI Reference Model. The OSI Reference Model is the highest level of abstraction in the OSI scheme. The paper first describes the basic building blocks used to construct the network model. Then the particular seven-layer model used by OSI is briefly described, followed by a discussion of outstanding issues and future extensions for the model.},
  keywords = {ARPANET,Communication standards,Computer networks,ISO standards,Open systems,Packet switching,Protocols,Springs,Standards development,Standards organizations}
}










